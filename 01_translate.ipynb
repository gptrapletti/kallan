{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import numpy as np\n",
    "import langchain\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "import dotenv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "from utils import count_tokens\n",
    "\n",
    "_ = dotenv.load_dotenv(dotenv.find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = fitz.open('data/kallan.pdf')\n",
    "pdf.page_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pages to keep: from 3 to 211 (zero-indexed, extremes included)\n",
    "idxs_pages_to_keep = list(range(3, 211 + 1))\n",
    "len(idxs_pages_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397670"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text = ''\n",
    "for i, page in enumerate(pdf):\n",
    "    if i in idxs_pages_to_keep:\n",
    "        pdf_text += page.get_text()\n",
    "        \n",
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Det började med några egendomliga ord som den där underliga flickan sa.\n",
      "Hon talade om en skogskälla som hennes mor sett som barn och berättat\n",
      "om. Vad skulle det vara för en källa, och vad var det för märkvärdigt med\n",
      "den? hade han frågat. Och flickan sa att det varken kunde eller fick hon\n",
      "säga, för det var en hemlighet, och om hon i förtid förrådde den\n",
      "hemligheten skulle källan ifråga bara försvinna. Detta sällsamma tal väckte\n",
      "hans undran och intresse. Och han kunde inte tiga. Så han frågade va\n"
     ]
    }
   ],
   "source": [
    "print(pdf_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141446"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = count_tokens(pdf_text)\n",
    "total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.55, 2.83)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text-to-speech model ('tts-1')\n",
    "cost_tts_1k_tokens = 0.015 \n",
    "\n",
    "# GPT3.5 Turbo ('gpt-3.5-turbo-1106')\n",
    "cost_input_1k_tokens = 0.001\n",
    "cost_output_1k_tokens = 0.002\n",
    "\n",
    "# # GPT4 Turbo ('gpt-4-1106-preview')\n",
    "# cost_input_1k_tokens = 0.01\n",
    "# cost_output_1k_tokens = 0.03\n",
    "\n",
    "# Assuming same length of input and output (1 chunk in, 1 chunk out)\n",
    "total_cost_1 = (total_tokens / 1000) * (cost_input_1k_tokens + cost_output_1k_tokens + cost_tts_1k_tokens)\n",
    "\n",
    "# Assuming to feed more chunks in input for context (3 chunk in, 1 chunk out)\n",
    "total_cost_2 = (total_tokens / 1000) * (3 * cost_input_1k_tokens + cost_output_1k_tokens + cost_tts_1k_tokens)\n",
    "\n",
    "round(total_cost_1, 2), round(total_cost_2, 2)\n",
    "\n",
    "# GPT3.5: (2.55, 2.83)\n",
    "# GPT4: (7.78, 10.61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split in chunks corresponding to chapters\n",
    "pattern = r'\\d+\\n'\n",
    "chunks = re.split(pattern, pdf_text)\n",
    "chunks = [chunk for chunk in chunks if chunk != '']\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482, 5438.0, 7836)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunk sizes\n",
    "sizes = [count_tokens(chunk) for chunk in chunks]\n",
    "min(sizes), np.mean(sizes).round(), max(sizes)\n",
    "\n",
    "# Chunk size is well under the GPT models context limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test translation quality of a long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7991"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/example.txt', 'r') as file:\n",
    "    example = file.read()\n",
    "    \n",
    "count_tokens(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''Please translate the following excerpt from a novel into English. \n",
    "Aim to preserve the author's original style and word choice as closely as possible. \n",
    "Here is the excerpt: {text}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = langchain.llms.OpenAI(model='gpt-3.5-turbo', temperature=0, max_tokens=50000)\n",
    "# llm_chain = langchain.chains.LLMChain(\n",
    "#     llm=llm, \n",
    "#     prompt=langchain.prompts.PromptTemplate.from_template(prompt_template)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat = langchain.chat_models.openai.ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "# prompt = langchain.prompts.PromptTemplate.from_template(prompt_template)\n",
    "# llm_chain = langchain.chains.LLMChain(llm=chat, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo-1106\" # max 16K tokens\n",
    "# model_name = \"gpt-4-1106-preview\" # max 128K tokens (10 times the price!)\n",
    "\n",
    "chat = ChatOpenAI(model_name=model_name, temperature=0, max_tokens=None)\n",
    "prompt = langchain.prompts.PromptTemplate.from_template(prompt_template)\n",
    "llm_chain = LLMChain(llm=chat, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm_chain(inputs={'text': example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `gpt-3.5-turbo-1106` and `gpt-4-1106-preview` have output token limit equal to 4096, so it's not possible to translate a longer text. So shorter chunks are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4182"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for chunk in chunks:\n",
    "    sentences += chunk.split('.')\n",
    "    \n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sentences = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idxs = list(range(0, len(sentences), 10))\n",
    "# groups = []\n",
    "# for idx in idxs:\n",
    "#     start = 0 + idx\n",
    "#     end = n_sentences + idx\n",
    "#     # print(start, end)\n",
    "#     group = sentences[start:end] # (0, 10), (10, 20), (20, 30), ...)\n",
    "#     groups.append(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = [sentences[i:i + n_sentences] for i in range(0, len(sentences), n_sentences)]\n",
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minichunks = []\n",
    "for group in groups:\n",
    "    group_text = ''\n",
    "    for sentence in group:\n",
    "        group_text += sentence + '.' + ' '\n",
    "    \n",
    "    minichunks.append(group_text)\n",
    "    \n",
    "len(minichunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 693, 1841)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = [count_tokens(minichunk) for minichunk in minichunks]\n",
    "min(sizes), int(np.mean(sizes)), max(sizes)\n",
    "\n",
    "# 10: (95, 347, 1363)\n",
    "# 20: (124, 693, 1841)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''Please translate the following excerpt from a novel into English. \n",
    "Aim to preserve the author's original style and word choice as closely as possible.\n",
    "You must answer with just the translated text and nothing else.\n",
    "Here is the excerpt: {text}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo-1106\" # max 16K tokens\n",
    "# model_name = \"gpt-4-1106-preview\" # max 128K tokens (10 times the price!)\n",
    "\n",
    "chat = ChatOpenAI(model_name=model_name, temperature=0, max_tokens=None)\n",
    "prompt = langchain.prompts.PromptTemplate.from_template(prompt_template)\n",
    "llm_chain = LLMChain(llm=chat, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was this quality in Felix that she called carefulness. It had been a long time since she dared to think about the day when Carl-Gustaf slammed his fist on the dining table and demanded that she remain silent on a matter of right or wrong just because he had a different opinion than she did, for it was the only time in her married life that she felt its foundation waver beneath her. She met her husband's gaze steadily, but what she saw in it - hatred and unforgiveness - she hoped she would never have to see again. For if it happened again, she did not know if she could overcome herself. But today she had thought about it. Why? She thought about it now - and shivered - when she saw the two mismatched brothers walking together among the graves and trees in the cemetery. She thought about how while Carl-Gustaf had a mild and good-natured face that he only managed to make look stern with a certain effort, Felix had a naturally stern face, a pair of coolly observant granite-gray eyes under imposing, bushy black eyebrows, which he with difficulty transformed into something friendly and genial. In the end, they had both almost succeeded in reshaping their faces: over the years they had acquired some of the character they wanted to have. She stood by the window curtain and watched the serious, sternly stretched brothers. Now they stood solemnly sunk at the sight of their parents' grave; she could almost only guess that they were standing there, for they were half hidden by the trees - which were in full bloom and buzzing with bees. They were middle-aged now, the brothers, she thought. They had begun to look back, in a kind of common reconciliation and reflection, and in this she saw the best thing that had happened - since before that day. It was then that little Eugen came down the road, a little rascal of eight years. She opened the window, leaned out and called to him: Eugen, have you seen Jerk? No, Eugen hadn't. He had just come here thinking Jerk was home. After about half an hour, Carl-Gustaf came home alone. Felix had accompanied Eugen Agrell down to the village to visit the Grandien brothers. Well, well - what's the matter with you, little Olga? her husband said when she rushed up to him and kissed him, once, twice, three times. You look as if you've seen something terrible. What has happened?\n"
     ]
    }
   ],
   "source": [
    "output = llm_chain(inputs={'text': minichunks[54]})\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was this quality in Felix that she called gentleness. It had been a long time since she dared to think about the day when Carl-Gustaf slammed his fist on the coffee table and demanded that she remain silent on a matter of right or wrong just because he had a different opinion than she did, for it was the only time in her married life that she felt its foundation waver beneath her. She met her husband's gaze steadily, but what she saw in it - hatred and unforgiveness - she hoped she would never have to see again. For if it happened again, she did not know if she could overcome herself. But today she had thought about it. Why? She thought about it now - and shivered - when she saw the two mismatched brothers walking together among the graves and trees in the cemetery. She thought about how while Carl-Gustaf had a mild and good-natured face that he only managed to make look stern with a certain effort, Felix had a naturally stern face, a pair of coolly observant granite-gray eyes under imposing, bushy black eyebrows, which he struggled to transform into something friendly and genial. In the end, they had both almost succeeded in reshaping their faces: over the years, they had acquired some of the character they wanted to have. She stood at the window curtain and watched the serious, stern brothers. Now they stood solemnly absorbed at the sight of their parents' grave; she could almost only guess that they were there, for they were half hidden by the trees - which were in full bloom and buzzing with bees. They were middle-aged now, the brothers, she thought. They had begun to look back, in a kind of shared reconciliation and reflection, and in this she saw the best thing that had happened - since before that day. That was when little Eugen came down the road, a little rascal of eight years. She opened the window, leaned out, and called to him: Eugen, have you seen Jerk? No, Eugen hadn't. He had just come here thinking Jerk was at home. After about half an hour, Carl-Gustaf came home alone. Felix had accompanied Eugen Agrell down to the village to visit the Grandien brothers. Well, well - what's the matter with you, little Olga? her husband said when she rushed up to him and kissed him, once, twice, three times. You look as if you've seen something terrible. What has happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was this quality in Felix that she called carefulness. It had been a long time since she dared to think about the day when Carl-Gustaf slammed his fist on the dining table and demanded that she remain silent on a matter of right or wrong just because he had a different opinion than she did, for it was the only time in her married life that she felt its foundation waver beneath her. She met her husband's gaze steadily, but what she saw in it - hatred and unforgiveness - she hoped she would never have to see again. For if it happened again, she did not know if she could overcome herself. But today she had thought about it. Why? She thought about it now - and shivered - when she saw the two mismatched brothers walking together among the graves and trees in the cemetery. She thought about how while Carl-Gustaf had a mild and good-natured face that he only managed to make look stern with a certain effort, Felix had a naturally stern face, a pair of coolly observant granite-gray eyes under imposing, bushy black eyebrows, which he with difficulty transformed into something friendly and genial. In the end, they had both almost succeeded in reshaping their faces: over the years they had acquired some of the character they wanted to have. She stood by the window curtain and watched the serious, sternly stretched brothers. Now they stood solemnly sunk at the sight of their parents' grave; she could almost only guess that they were standing there, for they were half hidden by the trees - which were in full bloom and buzzing with bees. They were middle-aged now, the brothers, she thought. They had begun to look back, in a kind of common reconciliation and reflection, and in this she saw the best thing that had happened - since before that day. It was then that little Eugen came down the road, a little rascal of eight years. She opened the window, leaned out and called to him: Eugen, have you seen Jerk? No, Eugen hadn't. He had just come here thinking Jerk was home. After about half an hour, Carl-Gustaf came home alone. Felix had accompanied Eugen Agrell down to the village to visit the Grandien brothers. Well, well - what's the matter with you, little Olga? her husband said when she rushed up to him and kissed him, once, twice, three times. You look as if you've seen something terrible. What has happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [30:20<00:00,  8.67s/it]\n"
     ]
    }
   ],
   "source": [
    "translations = []\n",
    "for minichunk in tqdm(minichunks):\n",
    "    time.sleep(3)\n",
    "    output = llm_chain(inputs={'text': minichunk})\n",
    "    translations.append(output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translations_202312021800.json', 'w') as file:\n",
    "    json.dump(translations, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvmonitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
